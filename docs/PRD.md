# Legal AI Chatbot 프로젝트 PRD

## 1. 프로젝트 개요
*   **제품명**: Legal AI Chatbot
*   **비전**: 법률 지식이 부족한 일반인들도 쉽고 정확하게 법률 정보를 찾을 수 있도록, 고성능 벡터 검색과 검증 가능한 답변을 제공하는 AI 챗봇.
*   **핵심 가치**: "누구나 쉽게 접근하는 내 손안의 변호사." 법령/판례 데이터를 정교하게 분석하여, 어려운 법률 용어 대신 이해하기 쉬운 답변과 명확한 근거(조문/판례)를 제시함.

## 2. 핵심 목표
*   **시민 친화적 접근성 (Accessibility for Everyone)**: 복잡한 설치 과정 없이 누구나 웹/모바일 인터페이스를 통해 쉽게 질문하고 답변을 받을 수 있는 서비스 지향 (초기 개발은 로컬에서 시작하되, 서비스 확장을 고려).
*   **사용자 중심의 정확성 (User-Centric Accuracy)**: 사용자의 모호한 질문(자연어)을 법률적 문맥으로 이해하고, 가장 관련성 높은 조문과 판례를 찾아냄.
*   **투명한 근거 제공 (Transparent Grounding)**: AI 답변에 대해 "왜 이런 결과가 나왔는지" 원문 링크와 함께 설명하여, 일반인도 답변의 신뢰성을 직접 확인할 수 있도록 함.
*   **자동화된 평가 (Automated Evaluation)**: 검색 품질(Hit@K, MRR)과 생성 품질(인용률, 근거 기반성)을 측정하는 파이프라인 구축.

## 3. 기술 스택
| 구성 요소 | 기술 | 선정 이유 |
| :--- | :--- | :--- |
| **언어** | Python 3.10+ | AI/데이터 엔지니어링 표준 언어. |
| **인터페이스** | Streamlit | 빠른 프로토타이핑 및 "출처 문서" 패널 시각화 용이. |
| **Vector DB** | Chroma | 가볍고 로컬 친화적이며 메타데이터 필터링 지원이 우수함. |
| **LLM** | Ollama (Llama3 / Qwen2.5) | 고성능 로컬 인퍼런스 지원. |
| **임베딩** | bge-m3 or e5-mistral | 로컬 구동 가능한 최신 다국어/Dense 검색 모델. |
| **ETL** | Makefile / CLI (Typer) | 단순하고 명확한 파이프라인 단계 정의. |
| **평가** | Custom Rules + Ragas | 법률 도메인 정확도를 위한 맞춤형 지표. |

## 4. 시스템 아키텍처 및 데이터 파이프라인
시스템은 크게 **인덱싱(ETL)**과 **검색(Retrieval)** 두 가지 주요 워크플로우로 나뉩니다.

### 4.1 데이터 파이프라인 (인덱싱)
1.  **수집 (`ingest_api.py`)**:
    *   공공 법률 API에서 표준 법률 데이터(법령, 조문, 시행일)를 가져옵니다.
    *   Raw 응답을 JSON으로 저장합니다.
2.  **정규화 (`normalize.py`)**:
    *   Raw 데이터를 분석 가능한 구조(Parquet/CSV)로 변환합니다.
    *   엔티티: `Law`(법령), `Article`(조문), `Case`(판례).
3.  **벡터 인덱싱 (`index_docs.py`)**:
    *   **청킹(Chunking)**: 조문 단위 혹은 판례의 의미 단위로 텍스트를 분할합니다. 문맥 보존을 위해 Window Sliding 등을 적용할 수 있습니다.
    *   **메타데이터 부착**: 각 청크에 법령명, 조문번호, 시행일 등의 메타데이터를 태깅합니다.
    *   **임베딩 생성**: 텍스트를 벡터로 변환하여 Chroma DB에 저장합니다.

### 4.2 검색 파이프라인 (Chatbot RAG)
1.  **쿼리 분석**: 사용자 질문에서 키워드 및 메타데이터 필터 조건(예: 특정 법령)을 추출합니다.
2.  **Step A: 벡터 검색 (Candidate Generation)**:
    *   쿼리 임베딩과 유사한 문서를 Chroma에서 Top-K(예: 20개) 검색합니다.
    *   필요 시 메타데이터 필터링을 적용하여 범위를 좁힙니다.
3.  **Step B: 재순위화 (Reranking)**:
    *   Cross-Encoder 등을 활용하여 쿼리와 검색된 후보군 간의 관련성을 정밀 채점합니다.
    *   최종 Top-N 컨텍스트를 선정합니다.
4.  **생성 (Generation)**:
    *   선정된 컨텍스트(조문/판례 텍스트)를 프롬프트에 주입합니다.
    *   엄격한 인용 형식을 갖춘 답변을 생성하도록 LLM에 요청합니다.

## 5. 기능 요구사항
### 5.1 MVP 기능 (우선순위 1)
*   [ ] **CLI 파이프라인**: 수집, 정규화, 벡터 인덱싱을 위한 스크립트.
*   [ ] **벡터 검색 엔진**: 메타데이터 필터링을 지원하는 유사도 검색.
*   [ ] **Reranker**: 검색 정밀도 향상을 위한 재순위화 모듈.
*   [ ] **프론트엔드**: 다음을 포함한 간단한 Streamlit 앱:
    *   채팅 입력창.
    *   "참고 문헌(출처)" 사이드 패널.
*   [ ] **인용**: 답변은 반드시 `[법령명 제X조]` 형식을 포함해야 함.

### 5.2 확장 기능 (우선순위 2)
*   [ ] **하이브리드 검색 (Keyword)**: BM25 등 키워드 검색을 추가하여 명사 매칭 보강.
*   [ ] **평가 대시보드**: 테스트 셋에 대한 Hit@K 점수를 보여주는 간단한 CLI 출력.

## 6. 데이터 모델 (Schema)
### Document Chunk (Chroma Collection)
*   **Content**: 텍스트 내용 (조문 본문, 판례 요지 등)
*   **Metadata**:
    *   `type`: 'article' | 'case'
    *   `law_name`: 법령명
    *   `article_number`: 조문 번호
    *   `source_file`: 출처 파일명
    *   `date`: 시행일/선고일

## 7. 성공 지표
*   **검색 성능 (Retrieval Performance)**:
    *   **Hit@K**: 정답 조문이 상위 K개(예: 5개) 검색 결과 내에 존재하는가?
    *   **MRR (Mean Reciprocal Rank)**: 정답이 얼마나 상위에 위치하는가?
*   **답변 품질 (Answer Quality)**:
    *   **인용률 (Citation Rate)**: 주장이 유효한 출처 인용으로 뒷받침되는 비율.
    *   **근거 기반성 (Groundedness)**: 답변의 키워드가 검색된 컨텍스트에서 추적 가능한 비율.

## 8. 개발 로드맵
1.  **Day 1-2**: 데이터 API 클라이언트 & Chroma DB 설정.
2.  **Day 3**: 파이프라인 구현 (수집 -> 청킹 -> 임베딩 -> 적재).
3.  **Day 4**: 검색 로직 (Vector Search + Rerank) & 기본 평가 구현.
4.  **Day 5**: UI 구현 (Streamlit) & 데모 완성.
